# https://colab.research.google.com/drive/1y9-uMROBqH7NaTzEVevxPReFnL-TL4TL
"""compute_kmer_distributions

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y9-uMROBqH7NaTzEVevxPReFnL-TL4TL
"""

from typing import Any, List, Callable, Union

import numpy as np
from collections import OrderedDict
import functools
from itertools import product
from scipy.special import softmax, kl_div, rel_entr
import math
import operator
from scipy.spatial.distance import jensenshannon

#from tqdm.auto import tqdm
import tqdm
import sys
sys.path.append('../')
import outflag_2_nickname

import h5py
import torch
import os

import matplotlib.pyplot as plt

from PL_Models import *
from ranker import Ranker

import frechet
import Experiments

import argparse
parser = argparse.ArgumentParser()
parser.add_argument('--whichseqs',default='cumulative',type=str)
#parser.add_argument('--whichseqs',default='lastproposed',type=str)
#parser.add_argument('--whichseqs',default='Xtest',type=str)
#parser.add_argument('--do_avghamm',default='False',type=str)
#parser.add_argument('--do_kmers',default='False',type=str)
#parser.add_argument('--do_motifscan',default='False',type=str)
#parser.add_argument('--do_ydistrib',default='False',type=str)
parser.add_argument('--dont_remake',default='False',type=str)
parser.add_argument('--mode',default='motifscanLim',type=str)
#parser.add_argument('--mode',default='motifscanAll',type=str)
#parser.add_argument('--mode',default='ydistrib',type=str)
#parser.add_argument('--mode',default='kmers',type=str)
###parser.add_argument('--mode',default='cdf_ydistrib',type=str)
#parser.add_argument('--mode',default='unc',type=str)
#parser.add_argument('--mode',default='plot_meandistrib',type=str)
#parser.add_argument('--mode',default='plot_meanunc',type=str)
#parser.add_argument('--mode',default='percidentity',type=str)
parser.add_argument('--kmer_length',default=3,type=int)
#parser.add_argument('--exp_list',default=['papJRZ','papJLZ','papCostJ','papJ130K10K'])
parser.add_argument('--nicks',nargs='+',default=[
    "JRZtNzq1M","JRZmNzq1M","JRZsNzq1M","JRZFNzq1M", "JRZBNzq1M", #,"JRZENzq1M","JRZzNzq1M","JRZxNzq1M"],
    "JLZtNzq1M","JLZmNzq1M","JLZsNzq1M","JLZFNzq1M", #,"JLZENzq1M","JLZxNzq1M"],

    "JRZtNMq5M","JRZmNMq5M",
    "JRPtNcqpM","JRPmNcqpM","JRPsNcqpM","JRPFNcqpM"
])

"""
python characterize_seqs.py --mode kmers

python characterize_seqs.py --mode unc --whichseqs Xtest

python characterize_seqs.py --mode plot_meandistrib --whichseqs cumulative #for MeanOfDistrib
python characterize_seqs.py --mode plot_meanunc --whichseqs lastproposed #for MeanOfUnc
"""


##########################################################################################################################################################################################################################################################################################################################################

def DNA(length):
    return ''.join(np.random.choice(['A','C','G','T']) for _ in range(length))

def one_hot_encode(
        sequence: str,
        alphabet: str = "ACGT",
        neutral_alphabet: str = "N",
        neutral_value: Any = 0,
        dtype=np.float32
    ) -> np.ndarray:
    """One-hot encode sequence."""

    def to_uint8(s):
        return np.frombuffer(s.encode("ascii"), dtype=np.uint8)

    lookup = np.zeros([np.iinfo(np.uint8).max, len(alphabet)], dtype=dtype)
    lookup[to_uint8(alphabet)] = np.eye(len(alphabet), dtype=dtype)
    lookup[to_uint8(neutral_alphabet)] = neutral_value
    lookup = lookup.astype(dtype)
    return lookup[to_uint8(sequence)]

class kmer_featurization:

    def __init__(self, k):
        """
        seqs: a list of DNA sequences
        k: the "k" in k-mer
        """
        self.k = k
        self.letters = ['A', 'C', 'G', 'T']
        self.multiplyBy = 4 ** np.arange(k-1, -1, -1) # the multiplying number for each digit position in the k-number system
        self.n = 4**k # number of possible k-mers

    def obtain_kmer_feature_for_a_list_of_sequences(self, seqs, write_number_of_occurrences=False):
        """
        Given a list of m DNA sequences, return a 2-d array with shape (m, 4**k) for the 1-hot representation of the kmer features.
        Args:
          write_number_of_occurrences:
            a boolean. If False, then in the 1-hot representation, the percentage of the occurrence of a kmer will be recorded; otherwise the number of occurrences will be recorded. Default False.
        """
        kmer_features = []
        for seq in seqs:
            this_kmer_feature = self.obtain_kmer_feature_for_one_sequence(seq.upper(), write_number_of_occurrences=write_number_of_occurrences)
            kmer_features.append(this_kmer_feature)

        kmer_features = np.array(kmer_features)

        return kmer_features

    def obtain_kmer_feature_for_one_sequence(self, seq, write_number_of_occurrences=False):
        """
        Given a DNA sequence, return the 1-hot representation of its kmer feature.
        Args:
          seq:
            a string, a DNA sequence
          write_number_of_occurrences:
            a boolean. If False, then in the 1-hot representation, the percentage of the occurrence of a kmer will be recorded; otherwise the number of occurrences will be recorded. Default False.
        """
        number_of_kmers = len(seq) - self.k + 1

        kmer_feature = np.zeros(self.n)

        for i in range(number_of_kmers):
            this_kmer = seq[i:(i+self.k)]
            this_numbering = self.kmer_numbering_for_one_kmer(this_kmer)
            kmer_feature[this_numbering] += 1

        if not write_number_of_occurrences:
            kmer_feature = kmer_feature / number_of_kmers

        return kmer_feature

    def kmer_numbering_for_one_kmer(self, kmer):
        """
        Given a k-mer, return its numbering (the 0-based position in 1-hot representation)
        """
        digits = []
        for letter in kmer:
            digits.append(self.letters.index(letter))

        digits = np.array(digits)

        numbering = (digits * self.multiplyBy).sum()

        return numbering

def convert_one_hot_to_ACGT(X, dna_dict = {0: "A", 1: "C", 2: "G", 3: "T"}):
    # convert one hot to A,C,G,T
    seq_list = []

    #print(f"AC {X=}")
    #for index in tqdm.tqdm(range(len(X)),desc='Convert one hot to ACGT'):
    for index in range(len(X)):
        seq = X[index]
        #for i in seq:
        #    print(f"{i.shape=}")
        seq_list += ["".join([dna_dict[np.where(i)[0][0]] for i in seq])]
    return seq_list

def compute_kmer_spectra(X, kmer_length=3, dna_dict = {0: "A", 1: "C", 2: "G", 3: "T"}):
    """
    # convert one hot to A,C,G,T
    seq_list = []

    #print(f"AC {X=}")
    for index in tqdm.tqdm(range(len(X))):
        seq = X[index]
        #for i in seq:
        #    print(f"{i.shape=}")
        seq_list += ["".join([dna_dict[np.where(i)[0][0]] for i in seq])]
    """
    seq_list=convert_one_hot_to_ACGT(X,dna_dict)
    #print(f"AC {seq_list=}")

    obj = kmer_featurization(kmer_length)  # initialize a kmer_featurization object
    kmer_features = obj.obtain_kmer_feature_for_a_list_of_sequences(seq_list, write_number_of_occurrences=True)

    kmer_permutations = ["".join(p) for p in product(["A", "C", "G", "T"], repeat=kmer_length)]

    kmer_dict = {}
    for kmer in kmer_permutations:
        n = obj.kmer_numbering_for_one_kmer(kmer)
        kmer_dict[n] = kmer

    global_counts = np.sum(np.array(kmer_features), axis=0)

    # what to compute entropy against
    #print(f"{global_counts=}")
    global_counts_normalized = global_counts / sum(global_counts) # this is the distribution of kmers in the testset

    return global_counts_normalized


def get_hamming_AC(seqs): #from pool_gen
    """
    May 15 2024: last: not enough memory, needs to be parallelized
    """
    """ 
    Not backpropagable bc inherenlty not single-sequence (which is what a DL needs as input)?
    NO! Inherently not backpropagable because of operator == 
    """
    match = seqs[None,::] == seqs[:,None,::] # batch x batch x 4 x L array with True/False entries
    matching_nucs = match.all(axis=-2) # batch x batch x L : whether or not the nt matches
    hamm_pairs = matching_nucs.sum(axis=-1) # sum over length: i.e. number of matches

    fraction_match = hamm_pairs/seqs.shape[-1] # turns into a percentage (batch x batch)
    return fraction_match # L by L (diagonal is 1s)

def get_hamming_AC_nomemoryshortage(seqs): #from pool_gen
    """ 
    Not backpropagable bc inherenlty not single-sequence (which is what a DL needs as input)?
    NO! Inherently not backpropagable because of operator == 
    """
    """
    n_seqs, nts, pos =  to_mutate.shape
    # batch_uncs = []
    batch_sails = []
    [m.zero_grad() for m in ranker.Predictive_Models]
    ##print(f"AACC: {len(to_mutate)=} {seqs=} {ranker.batch_size=}")
    for batch in np.split(to_mutate, ceil(seqs/ranker.batch_size)):
    """
    n_seqs, nts, pos =  seqs.shape
    batch_size=100
    """
    #To test: x=torch.tensor([[[0, 1, 1, 0, 1], [1, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 1, 0]]])
    z=torch.tensor([[[0, 1, 1, 0, 1], [1, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 1, 0]], [[0, 1, 1, 1, 1], [1, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]])
    PROSS: what are z[None,::] and z[:,None,::] ? Print them
    """
    for batch in np.split(seqs, math.ceil(n_seqs/batch_size)):
        match = seqs[None,::] == seqs[:,None,::] # batch x batch x 4 x L array with True/False entries
        matching_nucs = match.all(axis=-2) # batch x batch x L : whether or not the nt matches
    hamm_pairs = matching_nucs.sum(axis=-1) # sum over length: i.e. number of matches

    fraction_match = hamm_pairs/seqs.shape[-1] # turns into a percentage (batch x batch)
    return fraction_match # L by L (diagonal is 1s)


def calculate_cross_sequence_identity_batch(X1a, X2a, batch_size): # https://github.com/niralisomia/Diffusion_Small_Data/blob/main/seq_evals_improved.py
    """
    does it make sense for X1==X2?
    """
    if type(X1a)==torch.tensor:
        X1=np.array(X1a.numpy())
        X2=np.array(X2a.numpy())
    else:
        X1=X1a
        X2=X2a
    num_1, seq_length, alphabet_size = X1.shape    
    num_2 = X2.shape[0]
    X1 = np.reshape(X1, [-1, seq_length * alphabet_size])
    X2 = np.reshape(X2, [-1, seq_length * alphabet_size])
    #seq_identity = np.zeros((num_1, num_2)).astype(np.int8)
    seq_identity = np.zeros((num_1, num_2)) #23 Oct 2024: Amber recommend removing the astype(int8)
    #for start_idx in tqdm.tqdm(range(0, num_1, batch_size)):
    for start_idx in range(0, num_1, batch_size):
        end_idx = min(start_idx + batch_size, num_1)
        batch_result = np.dot(X1[start_idx:end_idx], X2.T) # chatgpt: the dot product measures similarity by counting matching positions (in the one-hot encoding sense) between sequences. So: each entry is the number of nts that are identical
        #seq_identity[start_idx:end_idx, :] = batch_result.astype(np.int8)    
        seq_identity[start_idx:end_idx, :] = batch_result    #23 Oct 2024: Amber recommend removing the astype(int8)
    return seq_identity

def percid_functions(X1, X2, batch_size=256):
    # AC WARNING: the outputs has not yet normalized by sequence length, but it should be (Yagi)
    percent_identity_1 = calculate_cross_sequence_identity_batch(X1, X2, batch_size)
    max_percent_identity_1 = np.max(percent_identity_1, axis=1)

    meanA = np.mean(percent_identity_1, axis=1)
    mean_axis1=np.mean(meanA)
    std_axis1=np.std(meanA)

    mean_percent_identity_1 = np.max(percent_identity_1.flatten())
    
    average_max_percent_identity_1 = np.mean(max_percent_identity_1)
    global_max_percent_identity_1 = np.max(max_percent_identity_1) 
    return max_percent_identity_1, \
           average_max_percent_identity_1, \
           global_max_percent_identity_1, \
           mean_percent_identity_1, \
           mean_axis1, \
           std_axis1 # max_percent_identity=array([17., 17., 15.]) average_max_percent_identity=16.333333333333332 global_max_percent_identity=17.0
# https://github.com/niralisomia/Diffusion_Small_Data/blob/main/run_functional_sequence_similarity.py

def sequence_diversity(x, batch_size):
    percent_identity = calculate_cross_sequence_identity_batch(x, x, batch_size)
    val = []
    for i in range(len(percent_identity)):
        sort = np.sort(percent_identity[i])[::-1] 
        #print(sort)
        val.append(sort[1]) # <-- take second highest percent identity due to match w/ self
    #print(val)
    return np.array(val)

def sequence_diversity_onerow(x, batch_size):
    percent_identity = calculate_cross_sequence_identity_batch(x, x, batch_size)
    val = []
    i=0
    sort = np.sort(percent_identity[i])[::-1] 
    return sort

##########################################################################################################################################################################################################################################################################################################################################




if __name__=='__main__':
    args = parser.parse_args()
    print(f"{args=}")

    whichseqs=args.whichseqs

    #i_al_loop=[1,4]
    #i_al_loop=[0,1,2,3,4]
    i_al_loop=['pristine','0','1','2','3','4']
    #index_from_loop=[42]
    index_from_loop=[41,42,43,44,45]

    dont_remake=False
    if args.dont_remake=='True': dont_remake=True

    #exp_list=args.exp_list
    exp_list=['dummy']
    # exp_list=[
    #         'Quick',
    #         ##'papKRZ', 

    #         #'papJRZ', 
    #         #'papQRZ', 
    #         #'papXRZ', 
            
    #         #'papJLZ',
    #         #'papXLZ',
    #         #'papQLZ',

    #         #'papCostJ',
    #         #'papJ130K10K',
    #     ] #['papJRZ'] #,'papJLZ','papQRZ']
    #if args.mode=='motifscanAll':
    #    exp_list=[
    #        #'papJLZ',
    #        #'papCostJ',
    #        'papJ130K10K',
    #    ]

    #kmer_length=3
    #kmer_length=4
    #kmer_length=5
    #kmer_length=6
    kmer_length=args.kmer_length

    if args.mode=='motifscanAll':
        motif_indexes_to_consider_all=list(range(879)) # grep MOTIF JASPAR2024_CORE_vertebrates_non-redundant_pfms_meme.txt | wc -l
        indexflag='indx-all'
    elif args.mode=='motifscanLim':       
        #motif_indexes_to_consider=[6,62,408,409,410]
        motif_indexes_to_consider_hepg2=[6,62,408,409,410,22,453,514,2,23,516,29,297,314,321,331,365]
        motif_indexes_to_consider_k562=[378,22,453,514,2,23,516,29,297,314,321,331,365]
        #indexflag='indx-'+('-'.join([str(ind) for ind in motif_indexes_to_consider]))
        indexflag='indx-lim'
    else:
        indexflag='dummy'

    ##############

    nicks=args.nicks
    if type(nicks)!=list:
        nicks=[nicks]

    # nicks={

    #     ##'Quick':['JRZzNzq1M'],
    #     #'Quick':['JRZFNzq1M',"QRZsNzq1M","QRZmNzq1M","QRZtNzq1M","QRZFNzq1M"],
    #     'Quick':['JRZBNzq1M'],

    #     ##'papKRZ':["KRZsNzq1M"],

    #     'papJRZ':["JRZtNzq1M","JRZmNzq1M","JRZsNzq1M","JRZFNzq1M", "JRZBNzq1M"], #,"JRZENzq1M","JRZzNzq1M","JRZxNzq1M"],
    #     'papJLZ':["JLZtNzq1M","JLZmNzq1M","JLZsNzq1M","JLZFNzq1M"], #,"JLZENzq1M","JLZxNzq1M"],

    #     'papQRZ':["QRZtNzq1M","QRZmNzq1M","QRZsNzq1M","QRZFNzq1M"], #,"QRZENzq1M","QRZxNzq1M"],
    #     'papQLZ':["QLZtNzq1M","QLZmNzq1M","QLZsNzq1M","QLZFNzq1M"], #,"QLZENzq1M","QLZxNzq1M"],

    #     'papXRZ':["XRZtNzq1M","XRZmNzq1M","XRZsNzq1M","XRZFNzq1M"], #,"XRZENzq1M","XRZxNzq1M"],
    #     'papXLZ':["XLZtNzq1M","XLZmNzq1M","XLZsNzq1M","XLZFNzq1M"], #,"XLZENzq1M","XLZxNzq1M"],


    #     'papCostJ':["JRZtNMq5M","JRZmNMq5M",    "JRZtNzq1M","JRZmNzq1M","JRZsNzq1M"],
    #     'papJ130K10K':["JRPtNcqpM","JRPmNcqpM","JRPsNcqpM","JRPFNcqpM"], #,"JRPxNcqpM"],

    #     }
    # if args.mode=='motifscanAll':
    #     nicks={
    #     'papJLZ':["JLZtNzq1M","JLZmNzq1M","JLZsNzq1M"], #,"JLZENzq1M","JLZxNzq1M"],
    #     'papCostJ':["JRZtNMq5M","JRZmNMq5M"],
    #     'papJ130K10K':["JRPtNcqpM","JRPmNcqpM","JRPsNcqpM","JRPFNcqpM"], #,"JRPxNcqpM"],
    #     }


    if os.uname()[1]=='auros':
        mydir='../../outputs_DALdna_4plot/SAFECOPY_outputs_DALdna_4plot_22Apr2024/'
    elif os.uname()[1]=='citra':
        mydir='../../imported_outputs_DALdna_4plot/'
    else:
        mydir='../../outputs_DALdna_4plot/'
    outpdir=mydir

    #for experim in exp_list:
    for experim in tqdm.tqdm(exp_list, total=len(exp_list)):
        print("============================================",experim)
        #nickloop=nicks[experim] 
        nickloop=nicks
        # if args.mode=='plot_meandistrib':
        #     mean_of_distrib={}
        #     errbmean_of_distrib={}
        #     std_of_distrib={}
        #     errbstd_of_distrib={}
        # if args.mode=='plot_meanunc':
        #     mean_of_unc={}
        #     errbmean_of_unc={}
        #     std_of_unc={}
        #     errbstd_of_unc={}

        #print(nickloop)   
        for nick in nickloop:
            #print("HHEREac")
            # if args.mode=='plot_meandistrib':
            #     #mean_of_distrib_s=[[] for _ in range(len(index_from_loop))] #[]
            #     mean_of_distrib_s=[[] for _ in range(len(i_al_loop))] #[]
            #     std_of_distrib_s=[[] for _ in range(len(i_al_loop))] #[]
            # if args.mode=='plot_meanunc':
            #     mean_of_unc_s=[[] for _ in range(len(i_al_loop))] #[]
            #     std_of_unc_s=[[] for _ in range(len(i_al_loop))] #[]
            #     #std_of_distrib_s=[[] for _ in range(len(index_from_loop))] #[]
            outflag=outflag_2_nickname.get_outflag_for_nick(nick).replace('mtrt-0.','mtrt-0p').replace('mtrt-1.','mtrt-1p')     
            #./Launch_DAL.sh CHOSEN_MODEL WHAT_DATASET+labels-seed0_+INIT_DISTRIB+_+INIT_NUMBER SUBSEL SEQ_METHOD 1 GENU NICK 100 mc_dropout_5 PRE_SEL HMNBAO MUTRATE | grep qsub
            #./Launch_DAL.sh ResidualBind VTS1_rnacompete2009labels-seed0_random0_5000 no-0.0-no-0.0-no-0.0 mutation 1 5000 VR5mNlt1M 100 mc_dropout_5 no 50 0.1 | grep qsub
            
            chosen_model=os.popen("python ../outflag_2_nickname.py --nick "+nick+" | tail -1 | awk '{print $2}'").read().replace('\n','')
            chosen_dataset=os.popen("python ../outflag_2_nickname.py --nick "+nick+" | tail -1 | awk '{print $3}'").read().replace('\n','')
            #genU=int(os.popen("python ../outflag_2_nickname.py --nick "+nick+" | tail -1 | awk '{print $7}'").read().replace('\n','')) # pre 11 oct 2024
            genU=int(os.popen("python ../outflag_2_nickname.py --nick "+nick+" | tail -1 | awk '{print $7}'").read().replace('\n','').replace('notanch-','')) 
            #print(f"{chosen_model=} {chosen_dataset=} {genU=}")
            for model_index in [0]:
                for i_f,index_from in enumerate(index_from_loop):
                    #print("HHERE1")
                    if whichseqs=='cumulative':
                        Xs=torch.empty(0)
                        Ys=torch.empty(0)
                    #for i_al in tqdm.tqdm(i_al_loop, desc='iAL Loop', colour='blue'):
                    for ii_al,i_al in enumerate(i_al_loop):

                        already_made=False
                        if args.mode=='avghamm':
                            avghammf=outpdir+outflag+"/avghamm_"+whichseqs+"_"+nick+"_"+str(index_from)+"_iAL-"+str(i_al)+".npy"
                            if os.path.isfile(avghammf) and dont_remake: 
                                already_made=True
                        if args.mode=='kmers':
                            kmerf=outpdir+outflag+"/kmerkldiv-"+str(kmer_length)+"_"+whichseqs+"_"+nick+"_"+str(index_from)+"_iAL-"+str(i_al)+".npy"
                            kmerfJ=outpdir+outflag+"/kmerjsdis-"+str(kmer_length)+"_"+whichseqs+"_"+nick+"_"+str(index_from)+"_iAL-"+str(i_al)+".npy"
                            kmerf1=outpdir+"/kmerdist-"+str(kmer_length)+"_"+whichseqs+"_"+nick+"_"+str(index_from)+"_iAL-"+str(i_al)+".npy"
                            if os.path.isfile(kmerf) and dont_remake: 
                                already_made=True
                        if 'motifscan' in args.mode:
                            memef=outpdir+outflag+"/motifscan"+"_"+indexflag+'_'+whichseqs+"_"+nick+"_"+str(index_from)+"_iAL-"+str(i_al)+".npy"
                            memef1=outpdir+outflag+"/motifscan1"+"_"+indexflag+'_'+whichseqs+"_"+nick+"_"+str(index_from)+"_iAL-"+str(i_al)+".npy"
                            if os.path.isfile(memef) and dont_remake: 
                                already_made=True

                        #if not already_made:
                        #if os.path.isdir(mydir+outflag):
                        if os.path.isdir(mydir+outflag) and not already_made:
                            if i_al=='pristine':
                                input_h5_file=mydir+outflag+"/dal_dataset_pristine_seed-0-0.h5" 
                            else:
                                input_h5_file=mydir+outflag+"/dal_dataset_"+str(index_from)+"_proposed_iAL-"+str(i_al)+".h5" 
                            data = h5py.File(input_h5_file, 'r')
                            if whichseqs=='Xtest':
                                Xs=torch.tensor(np.array(data['X_test']))
                                Ys=np.array(data['Y_test']) #AC it was Y_train until 8aug2024! So all stuff on Xtest up till that date is wrong!!!
                                if args.mode!='unc':
                                    if i_al!='pristine':
                                        uncf=mydir+'uncall_'+whichseqs+'_'+nick+'_seedadd-'+str(index_from)+'_iAL-'+str(i_al)+'.npy'
                                        uncs=np.load(uncf) #
                            elif whichseqs=='lastproposed':
                                Xs=torch.tensor(np.array(data['X_train']))
                                Ys=np.array(data['Y_train'])
                                if args.mode!='unc':
                                    if i_al!='pristine':
                                        uncf=mydir+'uncall_'+whichseqs+'_'+nick+'_seedadd-'+str(index_from)+'_iAL-'+str(i_al)+'.npy'
                                        uncs=np.load(uncf) #
                            elif whichseqs=='cumulative':
                                Xs=torch.cat((Xs,torch.tensor(np.array(data['X_train']))),axis=0)
                                Ys=np.concatenate((Ys,np.array(data['Y_train']).squeeze(1)),axis=0)
                            else:
                                print("ERROR: wrong whichseqs")

                            #if whichseqs=='cumulative':
                            #    print(f"{nick=} {i_al=} {Xs.shape=}")

                            # if args.mode=='plot_meandistrib':
                            #     mean_of_distrib_s[ii_al].append(np.mean(Ys))
                            #     std_of_distrib_s[ii_al].append(np.std(Ys))
                            # if args.mode=='plot_meanunc':
                            #     if whichseqs!='cumulative' and i_al!='pristine':
                            #         mean_of_unc_s[ii_al].append(np.mean(uncs))
                            #         std_of_unc_s[ii_al].append(np.std(uncs))

                            if 'ydistrib' in args.mode:
                                ydata=np.array(Ys)
                                print("AC: range [-5,12] so that hists are comparable. But range may not adapt well to every chosen dataset")
                                hist,bin_edges=np.histogram(ydata, bins=100, range=[-5.,12.], density=False) #, cumulative=cdf_wanted) #needs to be false and then divided manually in plot_distrib_AL.py
                                bin_centers=[]
                                for i in range(len(bin_edges)-1):
                                    bin_centers.append((bin_edges[i]+bin_edges[i+1])/2)
                                bin_centers=np.array(bin_centers)
                                np.save(outpdir+'yyhist_dal_dataset_'+nick+'_'+str(index_from)+'_'+str(i_al)+'.npy',hist)
                                np.save(outpdir+'yybins_dal_dataset_'+nick+'_'+str(index_from)+'_'+str(i_al)+'.npy',bin_centers)
                                """
                                AC: for plotting: plot_distrib_AL.py
                                """

                            #if args.mode=='zeroedseqs':


                            if args.mode=='unc':
                                if i_al!='pristine':
                                    #print("AC: WARNING: This error: RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False.")
                                    #print("AC: WARNING: gets solved by doing qsub job_char_unc.sh")
                                    model=eval("PL_"+chosen_model+"(input_h5_file='"+input_h5_file+"', initial_ds=True)") #, extra_str='"+extra_str+"')") # QUIQUIURG doesnt change anything in terms of pred? it only counts the ckpt you use??
                                    ckptf=mydir+outflag+'/ckpt_'+str(model_index)+"_"+str(index_from)+"_ial-"+str(i_al)+".ckpt"
                                    model = model.load_from_checkpoint(ckptf, input_h5_file=input_h5_file)      
                                    ranker_sp = Ranker([model], #QUIQUI for deepensemble should change
                                        batch_size=100, #QUIQUI equal for all models?
                                        how_many_batches=1,
                                        uncertainty_method='mc_dropout_5', #QUIQUI
                                        diversity_method='no', 
                                        highpred_method='no', 
                                        uncertainty_weight=1.0,
                                        diversity_weight=0.0, 
                                        highpred_weight=0.0, 
                                        chosen_model=chosen_model, 
                                        cycle=1, sigmadistr_freq=1, device='cuda', 
                                        outdir=outpdir,  #???
                                        outflag=outflag,   
                                        local_seed=0, #????           #self.seed_add+222*self.current_i_AL, 
                                        task_type=model.task_type)   
                                    print(f"{i_al=} {Xs.shape=}")
                                    unc_all=np.empty(0)
                                    preds_all=np.empty(0)
                                    for xbatch in np.array_split(Xs, math.ceil(len(Xs)/ranker_sp.batch_size)):
                                        unc_x, preds_x_ = ranker_sp.calculate_desiderata(xbatch, keep_grads = True)
                                        unc_all=np.concatenate((unc_all,unc_x.detach().cpu().numpy()),axis=0)
                                        preds_all=np.concatenate((preds_all,preds_x_.detach().cpu().numpy()),axis=0)
                                    print(f"UNCALL DEBUG: {whichseqs=} {i_al=} {nick=} {index_from=} {unc_all.shape=} {preds_all.shape=}")
                                    np.save(outpdir+'uncall_'+whichseqs+'_'+nick+'_seedadd-'+str(index_from)+'_iAL-'+str(i_al)+'.npy',np.array(unc_all)) 
                                    np.save(outpdir+'predsall_'+whichseqs+'_'+nick+'_seedadd-'+str(index_from)+'_iAL-'+str(i_al)+'.npy',np.array(preds_all)) 
                                    print("DONE:",outpdir+'uncall_'+whichseqs+'_'+nick+'_seedadd-'+str(index_from)+'_iAL-'+str(i_al)+'.npy') #added 19 Sept 2024

                            if args.mode=='avghamm':
                                #avghamm=torch.mean(get_hamming_AC(Xs)).numpy()
                                avghamm=np.mean(get_hamming_AC(Xs).numpy())
                                np.save(avghammf,avghamm)
                                print("DONE:",avghammf)

                            if args.mode=='percidentity':
                                """
                                AC: WARNING: this analysis makes npys that occupies A LOT of memory, tens of giga each file
                                """
                                #percent_identity = calculate_cross_sequence_identity_batch(Xs, Xs, batch_size=256) # pre 23 oct 2024
                                ###np.save(outpdir+'percidentity_'+whichseqs+'_'+nick+'_seedadd-'+str(index_from)+'_iAL-'+str(i_al)+'.npy',np.array(percent_identity)) 
                                # np.save(outpdir+'percidentitymean_'+whichseqs+'_'+nick+'_seedadd-'+str(index_from)+'_iAL-'+str(i_al)+'.npy',np.mean(percent_identity.flatten())) # pre 23 oct 2024
                                # np.save(outpdir+'percidentitystd_'+whichseqs+'_'+nick+'_seedadd-'+str(index_from)+'_iAL-'+str(i_al)+'.npy',np.std(percent_identity.flatten()))  # pre 23 oct 2024
                                max_percent_identity_1, average_max_percent_identity_1, global_max_percent_identity_1, mean_percent_identity_1, mean_axis1, std_axis1 =percid_functions(Xs, Xs, batch_size=256)
                                np.save(outpdir+'percidentitymax1_'+whichseqs+'_'+nick+'_seedadd-'+str(index_from)+'_iAL-'+str(i_al)+'.npy',average_max_percent_identity_1) # inserted 23 oct 2024
                                np.save(outpdir+'percidentitymean1_'+whichseqs+'_'+nick+'_seedadd-'+str(index_from)+'_iAL-'+str(i_al)+'.npy',mean_percent_identity_1) # inserted 23 oct 2024

                            if args.mode=='frechet':
                                device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
                                if i_al!='pristine':
                                    model=eval("PL_"+chosen_model+"(input_h5_file='"+input_h5_file+"', initial_ds=True)") #, extra_str='"+extra_str+"')") # QUIQUIURG doesnt change anything in terms of pred? it only counts the ckpt you use??
                                    ckptf=mydir+outflag+'/ckpt_'+str(model_index)+"_"+str(index_from)+"_ial-"+str(i_al)+".ckpt"
                                    model = model.load_from_checkpoint(ckptf, input_h5_file=input_h5_file) 
                                    Xs=Xs.to(device)
                                    frech=frechet.get_frechet_distance(model,Xs,Xs)
                                    np.save(outpdir+'frechet_'+whichseqs+'_'+nick+'_seedadd-'+str(index_from)+'_iAL-'+str(i_al)+'.npy',np.array(frech)) 

                            if args.mode=='kmers':
                                Xs1=Xs #.copy()
                                Xs_perm_numpy=Xs1.permute(0,2,1).numpy() # #QUIQUINONURG this should be good: try with x=torch.tensor([[[0,1,0,0,0],[1,0,1,0,0],[0,0,0,1,0],[0,0,0,0,1]]])
                                #for kmer_length in [2,3,4]:
                                print(f"{Xs_perm_numpy.shape=}")
                                #for kmer_length in [3]:
                                #    #kmer_length = 3
                                #    kmer_dist = compute_kmer_spectra(Xs_perm_numpy, kmer_length)
                                #    #print(f"{kmer_dist=} {kmer_dist.shape=}")
                                #    print(f"{kmer_length=} {kmer_dist.shape=}") # kmer_dist.shape : 4^kmer_length
                                kmer_dist = compute_kmer_spectra(Xs_perm_numpy, kmer_length) # kmer_dist.shape : 4^kmer_length
                                if i_al=='pristine':
                                    pristine_kmer_dist=kmer_dist

                                # Step 2: compute KLD (or JSD) between two distributions
                                #entropy = np.round(np.sum(kl_div(train_kmer_dist, synthetic_kmer_dist)), 3)
                                #entropy = np.sum(kl_div(kmer_dist, pristine_kmer_dist))
                                kldiv = kl_div(kmer_dist, pristine_kmer_dist)
                                #print(f"{entropy=}")
                                jsdis= jensenshannon(kmer_dist, pristine_kmer_dist) # https://www.biorxiv.org/content/10.1101/2024.05.23.595630v1.full.pdf https://github.com/niralisomia/Diffusion_Small_Data/blob/main/seq_evals_improved.py

                                #kmerf=mydir+outflag+"/kmer_"+whichseqs+"_"+nick+"_"+str(index_from)+"_iAL-"+str(i_al)+".npy"
                                #np.save(kmerf,kmer_dist)
                                np.save(kmerf,kldiv)
                                np.save(kmerfJ,jsdis)
                                #np.save(kmerf1,kmer_dist)
                                print("DONE:",kmerf)

                                """
                                PROSS: for plotting the matrix: inspiration: not useful: https://github.com/Genentech/polygraph/blob/main/tutorials/1_yeast_tutorial.ipynb
                                IDEAS:
                                - plot distance from KL of initial: si! se normalizzata, e se si fa cumulativamente, direi che si puo vedere che shift ha
                                - plot matrix as xaxis: but needs to be for a fixed kmer_length and it may be highly discontinuous
                                """


                            #print(f"{args.mode=} {nick=}")
                            if 'motifscan' in args.mode:
                                #print(f"{args.mode=} {nick=} booo")
                                Xs1=Xs #.copy()
                                Xs_perm_numpy=Xs1.permute(0,2,1).numpy() # #QUIQUINONURG this should be good: try with x=torch.tensor([[[0,1,0,0,0],[1,0,1,0,0],[0,0,0,1,0],[0,0,0,0,1]]])
                                # https://github.com/althonos/pymemesuite
                                #!pip install seqio
                                import Bio.SeqIO
                                from pymemesuite.common import Sequence
                                from pymemesuite.fimo import FIMO
                                from pymemesuite.common import MotifFile

                                #jaspf='./JASPAR2020_CORE_insects_redundant_pfms_meme.txt'
                                jaspf="./JASPAR2024_CORE_vertebrates_non-redundant_pfms_meme.txt" #Slack Chris
                                """Agarwal2023: JASPAR 2022 CORE vertebrate non-redundant database : 53. Castro-Mondragon, J. A. et al. JASPAR 2022: the 9th release of the open-access database of transcription factor binding profiles. Nucleic Acids Res. 50, D165-D173 (2022)."""
                                motifs=[]
                                with MotifFile(jaspf) as motif_file:
                                    background=motif_file.background
                                    for i in range(100000): # 879 in total
                                        motif=motif_file.read()

                                        if args.mode=='motifscanAll':
                                            motif_indexes_to_consider=motif_indexes_to_consider_all
                                        elif args.mode=='motifscanLim':  
                                            if nick[0]=='J':
                                                motif_indexes_to_consider=motif_indexes_to_consider_k562
                                            elif nick[0]=='Q':
                                                motif_indexes_to_consider=motif_indexes_to_consider_hepg2
                                            else:
                                                motif_indexes_to_consider=[]
    
                                        if motif is not None and i in motif_indexes_to_consider:
                                            motifs.append(motif)
                                        

                                seq_list=convert_one_hot_to_ACGT(Xs_perm_numpy)
                                fastaname='./prov.fasta'
                                fastaf=open(fastaname,'w')
                                for seq in seq_list:
                                    fastaf.write('>\n'+seq+'\n')
                                fastaf.close()

                                sequences = [
                                    Sequence(str(record.seq), 
                                    name=record.id.encode()
                                    )
                                    for record in Bio.SeqIO.parse(fastaname, "fasta")
                                ]
                                fimo = FIMO(both_strands=False)
                                total_n=0
                                score_sum=0.0
                                #for i_mt,motif in tqdm.tqdm(enumerate(motifs),total=len(motifs),desc='loop over motifs',colour='red'):
                                for i_mt,motif in enumerate(motifs):
                                    pattern = fimo.score_motif(motif, sequences, background)
                                    total_n+=len(pattern.matched_elements)
                                    for i_m,m in enumerate(pattern.matched_elements):
                                        score_sum+=m.score
                                np.save(memef,np.array(total_n))
                                np.save(memef1,np.array(score_sum))
                                print("DONE:",memef)

        #     if 'plot_mean' in args.mode: 
        #         if args.mode=='plot_meandistrib':
        #             mean_of_distrib[nick]=[]
        #             errbmean_of_distrib[nick]=[]
        #             std_of_distrib[nick]=[]
        #             errbstd_of_distrib[nick]=[]
        #             for ii_al,i_al in enumerate(i_al_loop):
        #                 mean_of_distrib[nick].append(np.mean(mean_of_distrib_s[ii_al]))
        #                 errbmean_of_distrib[nick].append(np.std(mean_of_distrib_s[ii_al]))
        #                 std_of_distrib[nick].append(np.mean(std_of_distrib_s[ii_al]))
        #                 errbstd_of_distrib[nick].append(np.std(std_of_distrib_s[ii_al]))
        #             mean_of_distrib[nick]=np.array(mean_of_distrib[nick])
        #             errbmean_of_distrib[nick]=np.array(errbmean_of_distrib[nick])
        #             std_of_distrib[nick]=np.array(std_of_distrib[nick])
        #             errbstd_of_distrib[nick]=np.array(errbstd_of_distrib[nick])
        #             print(f"CHECK FOR ZEROS: {mean_of_distrib[nick]=} {std_of_distrib[nick]=}  {nick=}")
        #         if args.mode=='plot_meanunc':
        #             if whichseqs!='cumulative':
        #                 mean_of_unc[nick]=[]
        #                 errbmean_of_unc[nick]=[]
        #                 std_of_unc[nick]=[]
        #                 errbstd_of_unc[nick]=[]
        #                 for ii_al,i_al in enumerate(i_al_loop):
        #                     if args.mode=='plot_meanunc':
        #                         if whichseqs!='cumulative':
        #                             mean_of_unc[nick].append(np.mean(mean_of_unc_s[ii_al]))
        #                             errbmean_of_unc[nick].append(np.std(mean_of_unc_s[ii_al]))
        #                             std_of_unc[nick].append(np.mean(std_of_unc_s[ii_al]))
        #                             errbstd_of_unc[nick].append(np.std(std_of_unc_s[ii_al]))
        #                 mean_of_unc[nick]=np.array(mean_of_unc[nick])
        #                 errbmean_of_unc[nick]=np.array(errbmean_of_unc[nick])
        #                 std_of_unc[nick]=np.array(std_of_unc[nick])
        #                 errbstd_of_unc[nick]=np.array(errbstd_of_unc[nick])

        # #if 'plot_mean' in args.mode:
        # if args.mode=='plot_meandistrib':
        #     fig=plt.figure(1, figsize=(6, 5))
        #     filename='MeanOfDistrib_'+experim+'_'+args.whichseqs+'.png'
        #     for nick in nicks[experim]:
        #         short_dict=outflag_2_nickname.make_short_dict(nick)
        #         if 'Cost' in experim and nick in ["JRZtNMq5M", "JRZmNMq5M"]:
        #             plt.plot(np.arange(len(mean_of_distrib[nick])),mean_of_distrib[nick],label=short_dict[nick]['plotname'],color=short_dict[nick]['color'],lw=1,linestyle='-.')
        #             plt.errorbar(np.arange(len(mean_of_distrib[nick])),mean_of_distrib[nick],yerr=errbmean_of_distrib[nick], ls='none', color=short_dict[nick]['color'])
        #         else:
        #             plt.plot(np.arange(len(mean_of_distrib[nick])),mean_of_distrib[nick],label=short_dict[nick]['plotname'],color=short_dict[nick]['color'],lw=1)
        #             plt.scatter(np.arange(len(mean_of_distrib[nick])),mean_of_distrib[nick],color=short_dict[nick]['color'])
        #             plt.fill_between(np.arange(len(mean_of_distrib[nick])),mean_of_distrib[nick]-errbmean_of_distrib[nick],mean_of_distrib[nick]+errbmean_of_distrib[nick],color=short_dict[nick]['color'],alpha=0.5)
        #         np.save(outpdir+"mean_of_distrib"+nick+"_"+args.whichseqs+".npy",mean_of_distrib[nick])
        #         if nick[0]=='X':
        #             plt.ylabel('Mean Oracle Binding Score')
        #         else:
        #             plt.ylabel('Mean Oracle CRE Activity') 
        #         plt.xlabel('Cycle')
        #     plt.plot(np.linspace(0.0,5.0,2),-0.150578*np.ones(2),color='black',linestyle='-.',label='Full Dataset') #GOODOLD
        #     plt.legend(loc='lower right',prop={'size':12})
        #     plt.ylim(-0.9,0.25)
        #     plt.xticks(np.arange(6),['0','1','2','3','4','5'])
        #     fig.savefig(outpdir+filename,dpi=200,bbox_inches='tight')
        #     print("DONE:",outpdir+filename)
        #     plt.clf()

        #     fig=plt.figure(1, figsize=(6, 5))
        #     filename='StdOfDistrib_'+experim+'_'+args.whichseqs+'.png'
        #     for nick in nicks[experim]:
        #         short_dict=outflag_2_nickname.make_short_dict(nick)
        #         if 'Cost' in experim and nick in ["JRZtNMq5M", "JRZmNMq5M"]:
        #             plt.plot(np.arange(len(std_of_distrib[nick])),std_of_distrib[nick],label=short_dict[nick]['plotname'],color=short_dict[nick]['color'],lw=1,linestyle='-.')
        #             plt.errorbar(np.arange(len(std_of_distrib[nick])),std_of_distrib[nick],yerr=errbstd_of_distrib[nick], ls='none', color=short_dict[nick]['color'])
        #         else:
        #             plt.plot(np.arange(len(std_of_distrib[nick])),std_of_distrib[nick],label=short_dict[nick]['plotname'],color=short_dict[nick]['color'],lw=1)
        #             plt.scatter(np.arange(len(std_of_distrib[nick])),std_of_distrib[nick],color=short_dict[nick]['color'])
        #             plt.fill_between(np.arange(len(std_of_distrib[nick])),std_of_distrib[nick]-errbstd_of_distrib[nick],std_of_distrib[nick]+errbstd_of_distrib[nick],color=short_dict[nick]['color'],alpha=0.5)
        #         if nick[0]=='X':
        #             plt.ylabel('Oracle Binding Score St.Dev.')
        #         else:
        #             plt.ylabel('Oracle CRE Activity St.Dev.') 
        #         plt.xlabel('Cycle')
        #     plt.legend(loc='lower right',prop={'size':12})
        #     plt.ylim(0.2,0.8)
        #     plt.xticks(np.arange(6),['0','1','2','3','4','5'])
        #     fig.savefig(outpdir+filename,dpi=200,bbox_inches='tight')
        #     print("DONE:",outpdir+filename)
        #     plt.clf()

        # if args.mode=='plot_meanunc':
        #     if whichseqs!='cumulative': # and i_al!='pristine':

        #         #print("NOOOO",mean_of_unc)
        #         fig=plt.figure(1, figsize=(6, 5))
        #         filename='MeanOfUnc_'+experim+'_'+args.whichseqs+'.png'
        #         for nick in nicks[experim]:
        #             short_dict=outflag_2_nickname.make_short_dict(nick)
        #             if 'Cost' in experim and nick in ["JRZtNMq5M", "JRZmNMq5M"]:
        #                 plt.plot(np.arange(len(mean_of_unc[nick])),mean_of_unc[nick],label=short_dict[nick]['plotname'],color=short_dict[nick]['color'],lw=1,linestyle='-.')
        #                 plt.errorbar(np.arange(len(mean_of_unc[nick])),mean_of_unc[nick],yerr=errbmean_of_unc[nick], ls='none', color=short_dict[nick]['color'])
        #             else:   
        #                 plt.plot(np.arange(len(mean_of_unc[nick])),mean_of_unc[nick],label=short_dict[nick]['plotname'],color=short_dict[nick]['color'],lw=1)
        #                 plt.scatter(np.arange(len(mean_of_unc[nick])),mean_of_unc[nick],color=short_dict[nick]['color'])
        #                 plt.fill_between(np.arange(len(mean_of_unc[nick])),mean_of_unc[nick]-errbmean_of_unc[nick],mean_of_unc[nick]+errbmean_of_unc[nick],color=short_dict[nick]['color'],alpha=0.5)
        #             if nick[0]=='X':
        #                 plt.ylabel('Mean Unc. on Binding Score')
        #             else:
        #                 plt.ylabel('Mean Unc. on CRE Activity') 
        #             plt.xlabel('Cycle')
        #         plt.legend(loc='lower right',prop={'size':12})
        #         #plt.xticks(['0','1','2','3','4','5'])
        #         plt.ylim(0.0,0.25)
        #         plt.xticks(np.arange(5),['1','2','3','4','5'])
        #         fig.savefig(outpdir+filename,dpi=200,bbox_inches='tight')
        #         print("DONE:",outpdir+filename)
        #         plt.clf()

        #         fig=plt.figure(1, figsize=(6, 5))
        #         filename='StdOfUnc_'+experim+'_'+args.whichseqs+'.png'
        #         for nick in nicks[experim]:
        #             short_dict=outflag_2_nickname.make_short_dict(nick)
        #             if 'Cost' in experim and nick in ["JRZtNMq5M", "JRZmNMq5M"]:
        #                 plt.plot(np.arange(len(std_of_unc[nick])),std_of_unc[nick],label=short_dict[nick]['plotname'],color=short_dict[nick]['color'],lw=1,linestyle='-.')
        #                 plt.errorbar(np.arange(len(std_of_unc[nick])),std_of_unc[nick],yerr=errbstd_of_unc[nick], ls='none', color=short_dict[nick]['color'])
        #             else:
        #                 plt.plot(np.arange(len(std_of_unc[nick])),std_of_unc[nick],label=short_dict[nick]['plotname'],color=short_dict[nick]['color'],lw=1)
        #                 plt.scatter(np.arange(len(std_of_unc[nick])),std_of_unc[nick],color=short_dict[nick]['color'])
        #                 plt.fill_between(np.arange(len(std_of_unc[nick])),std_of_unc[nick]-errbstd_of_unc[nick],std_of_unc[nick]+errbstd_of_unc[nick],color=short_dict[nick]['color'],alpha=0.5)
        #             if nick[0]=='X':
        #                 plt.ylabel('Binding Score Unc. St.Dev.')
        #             else:
        #                 plt.ylabel('CRE Activity Unc. St.Dev.') 
        #             plt.xlabel('Cycle')
        #         plt.legend(loc='lower right',prop={'size':12})
        #         plt.xticks(np.arange(5),['1','2','3','4','5'])
        #         fig.savefig(outpdir+filename,dpi=200,bbox_inches='tight')
        #         print("DONE:",outpdir+filename)
        #         plt.clf()



    print("SCRIPT END.")

    """
    pristine,
    AL-differently,
    whichseqs_differently
    divided by len?
    """

    """
    The top three TFBSs most frequently associated with transcriptional activation
    among all cell types corresponded to KLF5/15, NFYA/NFYC, and FOXI1/FOXJ2; in contrast,
    the top cell-type-specific TFBS corresponded to HNF4A/G in HepG2 cells

    grep -n HNF4 JASPAR2024_CORE_vertebrates_non-redundant_pfms_meme.txt 
5994:MOTIF MA1494.2 HNF4A
6012:MOTIF MA0114.5 HNF4A
6025:MOTIF MA0484.3 HNF4G


    MOTIF MA0114.5 HNF4A
letter-probability matrix: alength= 4 w= 9 nsites= 46504 E= 0
 0.031567  0.891171  0.026600  0.050662
 0.931554  0.027030  0.022815  0.018601
 0.864807  0.029460  0.077735  0.027998
 0.930329  0.011698  0.040792  0.017181
 0.015397  0.019697  0.936006  0.028901
 0.019934  0.014945  0.046964  0.918158
 0.059565  0.812919  0.039265  0.088250
 0.029309  0.910481  0.013827  0.046383
 0.788190  0.059823  0.063134  0.088853
URL http://jaspar.genereg.net/matrix/MA0114.5
    """


    """
    AC: This analysis in not as informative as y, motifs, or even avg hamm
    """






    """
        N, L, A = (500, 200, 4)

    # Step 0: create sample input
    train_ds = np.array([one_hot_encode(DNA(L)) for i in range(N)]) # N, L, A
    synthetic_ds = np.array([one_hot_encode(DNA(L)) for i in range(N)]) # N, L, A

    # Step 1: compute k-mer dist of train and synthetic dset
    kmer_length = 3
    train_kmer_dist = compute_kmer_spectra(train_ds, kmer_length)
    synthetic_kmer_dist = compute_kmer_spectra(synthetic_ds, kmer_length)

    # Step 2: compute KLD (or JSD) between two distributions

    entropy = np.round(np.sum(kl_div(train_kmer_dist, synthetic_kmer_dist)), 3)
    print(f"{entropy=}")

    # OR use JSD
    # add code here

    print(f"{train_kmer_dist=}")

    print(f"{synthetic_kmer_dist=}")




    #test to make sure the saved data is still the same as the original
    print(np.allclose(test, train_ds))

    #!pip install seqio
    from pymemesuite.common import Sequence
    from pymemesuite.fimo import FIMO
    from pymemesuite.common import MotifFile

    with MotifFile("JASPAR2024_CORE_vertebrates_non-redundant_pfms_meme.txt") as motif_file:
        motif = motif_file.read()
        motif2 = motif_file.read()

    print(motif.frequencies)

    for row in motif2.frequencies:
        print(" ".join(f'{freq:.2f}' for freq in row))

    
    """